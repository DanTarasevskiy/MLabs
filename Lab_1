#Градиентный спуск

import numpy as np
import matplotlib.pyplot as plt

coords_x = []
coords_y = []
coords_z = []

def sphere(x):
    return x[0]**2 + x[1]**2

def gradient_descent(start, learning_rate, num_iterations):
    trajectory = []
    x = start.copy()

    for _ in range(num_iterations):
        gradient = 2 * x
        x = x - learning_rate * gradient
        x[2] = sphere(x)
        trajectory.append(x.copy())
        coords_x.append(x[0])
        coords_y.append(x[1])
        coords_z.append(x[2])
        if _ % (num_iterations // 10) == 0:
            print(f'Итерация {_}: x = {x[0]}, y = {x[1]}, f(x) = {x[2]}, grad = {gradient}')

    return x, trajectory

start_point = np.array([-5, -5, 0])  
learning_rate = 0.1  # Скорость обучения
num_iterations = 100  

optimal_point, trajectory = gradient_descent(start_point, learning_rate, num_iterations)

# Визуализация функции и точки оптимума
x = np.linspace(-5, 5, 25)
y = np.linspace(-5, 5, 25)
X, Y = np.meshgrid(x, y)
Z = X**2 + Y**2

fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
ax.plot_surface(X, Y, Z, alpha=0.5)

trajectory = np.array(trajectory)
ax.scatter(coords_x, coords_y, coords_z, c='r', marker='o')
ax.scatter(optimal_point[0], optimal_point[1], sphere(optimal_point), c='g', marker='o')

plt.show()

